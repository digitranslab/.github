# ğŸ—ï¸ DBLOCK Architecture

<p align="center">
  <img src="https://img.shields.io/badge/Document-Architecture-8B5CF6?style=for-the-badge" alt="Architecture">
  <img src="https://img.shields.io/badge/Status-Coming%20Soon-3B82F6?style=for-the-badge" alt="Coming Soon">
</p>

<p align="center">
  <a href="../../README.md">ğŸ  Home</a> â€¢
  <a href="../../PRODUCTS.md">ğŸ“¦ Products</a> â€¢
  <a href="README.md">ğŸ“– Overview</a> â€¢
  <a href="FEATURES.md">âœ¨ Features</a> â€¢
  <a href="ROADMAP.md">ğŸ—ºï¸ Roadmap</a>
</p>

---

## Overview

This document describes the system architecture of **DBLOCK**, DigiTransLab's data workflow automation platform. DBLOCK is designed with a modular, scalable architecture that enables organisations to build, deploy, and manage sophisticated data pipelines through an intuitive visual interface.

---

## ğŸ“Š Pipeline Architecture Diagram

<p align="center">
  <img src="../../assets/diagrams/dblock-pipeline.svg" alt="DBLOCK Pipeline Architecture" width="800">
</p>

<p align="center">
  <em>Figure 1: DBLOCK Pipeline Architecture and Data Flow</em>
</p>

---

## ğŸ§© System Components

DBLOCK consists of several interconnected components that work together to provide comprehensive data workflow automation capabilities:

### Core Components

| Component | Description |
|-----------|-------------|
| **Visual Editor** | The central design interface that enables users to create and modify data pipelines through drag-and-drop interactions. Supports complex transformations, branching logic, and parallel processing without requiring code. |
| **Connector Hub** | Manages connections to 100+ data sources and destinations. Provides standardised interfaces for databases, data warehouses, cloud storage, APIs, and streaming platforms. |
| **Execution Engine** | The orchestration layer that executes data pipelines. Manages workflow state, handles error recovery, and coordinates data movement across integrated systems. |
| **AI Assistant** | Intelligent assistance module that provides pipeline design suggestions, transformation recommendations, and automated optimisation based on data patterns and usage. |

### Supporting Components

| Component | Description |
|-----------|-------------|
| **Data Catalogue** | Maintains metadata about data sources, schemas, and lineage. Enables discovery and understanding of available data assets across the organisation. |
| **Scheduler** | Manages pipeline execution schedules, triggers, and dependencies. Supports cron-based scheduling, event-driven triggers, and manual execution. |
| **Monitoring Dashboard** | Provides real-time visibility into pipeline execution, performance metrics, and data quality indicators. |
| **Configuration Manager** | Manages system settings, connection credentials, and pipeline configurations with secure storage and version control. |

---

## ğŸ”„ Data Flow

The following describes how data flows through the DBLOCK system from source ingestion to destination delivery:

### 1. Data Ingestion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Sources                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Databases  â”‚   Cloud     â”‚  Streaming  â”‚    APIs     â”‚  Files  â”‚
â”‚ (PostgreSQL,â”‚  Storage    â”‚   (Kafka,   â”‚   (REST,    â”‚  (CSV,  â”‚
â”‚   MySQL)    â”‚ (S3, GCS)   â”‚   Kinesis)  â”‚   GraphQL)  â”‚  JSON)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚             â”‚           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Connector Hub    â”‚
                        â”‚  (Standardisation)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
```

Data is collected from various sources through the **Connector Hub**, which standardises data formats and handles connection management for consistent processing.

### 2. Transformation Processing

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Execution Engine   â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ â€¢ Data Validation   â”‚
                        â”‚ â€¢ Transformations   â”‚
                        â”‚ â€¢ Enrichment        â”‚
                        â”‚ â€¢ Aggregation       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              â”‚              â”‚
                    â–¼              â–¼              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Filter  â”‚  â”‚   Join   â”‚  â”‚ Aggregateâ”‚
              â”‚  & Map   â”‚  â”‚  & Merge â”‚  â”‚ & Window â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   â”‚             â”‚             â”‚
```

The **Execution Engine** processes data through user-defined transformations, including filtering, mapping, joining, and aggregation operations.

### 3. AI-Assisted Optimisation

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    AI Assistant     â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ â€¢ Schema Inference  â”‚
                        â”‚ â€¢ Query Optimisationâ”‚
                        â”‚ â€¢ Pattern Detection â”‚
                        â”‚ â€¢ Recommendations   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              â”‚              â”‚
                    â–¼              â–¼              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Pipeline â”‚  â”‚  Data    â”‚  â”‚ Resource â”‚
              â”‚ Suggest  â”‚  â”‚ Quality  â”‚  â”‚ Scaling  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The **AI Assistant** analyses pipeline patterns and data characteristics to provide intelligent suggestions for optimisation and improvement.

### 4. Data Delivery

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Output Manager    â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ â€¢ Format Conversion â”‚
                        â”‚ â€¢ Batch Delivery    â”‚
                        â”‚ â€¢ Stream Publishing â”‚
                        â”‚ â€¢ API Exposure      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              â”‚              â”‚
                    â–¼              â–¼              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Data   â”‚  â”‚ Analyticsâ”‚  â”‚  Real-   â”‚
              â”‚Warehousesâ”‚  â”‚ Platformsâ”‚  â”‚  Time    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The **Output Manager** delivers processed data to destinations in the appropriate format, supporting batch loads, streaming outputs, and API endpoints.

---

## ğŸ”— Connector Architecture

DBLOCK's **Connector Hub** provides connectivity to 100+ data sources and destinations:

### Connector Categories

<p align="center">
  <img src="https://img.shields.io/badge/Databases-PostgreSQL%20|%20MySQL%20|%20MongoDB-3B82F6?style=flat-square" alt="Databases">
  <img src="https://img.shields.io/badge/Warehouses-Snowflake%20|%20BigQuery%20|%20Redshift-8B5CF6?style=flat-square" alt="Warehouses">
  <img src="https://img.shields.io/badge/Streaming-Kafka%20|%20Kinesis%20|%20Pub/Sub-3B82F6?style=flat-square" alt="Streaming">
</p>

| Category | Connectors | Purpose |
|----------|------------|---------|
| **Databases** | PostgreSQL, MySQL, MongoDB, Oracle, SQL Server | Transactional data extraction and loading |
| **Data Warehouses** | Snowflake, BigQuery, Redshift, Databricks | Analytics and reporting destinations |
| **Cloud Storage** | AWS S3, Azure Blob, Google Cloud Storage | File-based data ingestion and archival |
| **Streaming** | Apache Kafka, AWS Kinesis, Google Pub/Sub | Real-time event processing |
| **SaaS Applications** | Salesforce, HubSpot, Stripe, Shopify | Business application integration |
| **APIs** | REST, GraphQL, SOAP, Custom webhooks | External service connectivity |

### Connector Patterns

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Connector Hub                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   Batch     â”‚  â”‚  Streaming  â”‚  â”‚   Change    â”‚              â”‚
â”‚  â”‚  Connector  â”‚  â”‚  Connector  â”‚  â”‚   Data      â”‚              â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   Capture   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                â”‚                â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                 â”‚  Schema Registryâ”‚                              â”‚
â”‚                 â”‚  & Validation   â”‚                              â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Real-Time Streaming Architecture

DBLOCK supports real-time data processing with built-in streaming capabilities:

### Streaming Components

| Component | Description |
|-----------|-------------|
| **Stream Ingestion** | Consumes events from streaming platforms with configurable parallelism and offset management |
| **Window Processing** | Supports tumbling, sliding, and session windows for time-based aggregations |
| **State Management** | Maintains processing state for exactly-once semantics and fault tolerance |
| **Backpressure Handling** | Automatically manages flow control to prevent system overload |

### Streaming Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Real-Time Processing                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Event     â”‚â”€â”€â”€â”€â–¶â”‚   Stream    â”‚â”€â”€â”€â”€â–¶â”‚   Output    â”‚        â”‚
â”‚  â”‚   Source    â”‚     â”‚  Processor  â”‚     â”‚   Sink      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                             â”‚                                    â”‚
â”‚                             â–¼                                    â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                    â”‚  State Store    â”‚                           â”‚
â”‚                    â”‚  (Checkpoints)  â”‚                           â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ  Deployment Architecture

DBLOCK supports **self-hosted** and **cloud deployment** options for flexibility and control:

### Deployment Options

| Option | Description | Use Case |
|--------|-------------|----------|
| **Single Node** | All components on one server | Development, small teams |
| **Distributed** | Components across multiple nodes | Production environments |
| **Kubernetes** | Container orchestration deployment | Enterprise scale |
| **Managed Cloud** | Fully managed service (coming soon) | Hands-off operation |

### Infrastructure Requirements

<details>
<summary><strong>ğŸ“‹ Minimum Requirements</strong></summary>

| Resource | Specification |
|----------|---------------|
| **CPU** | 4 cores |
| **Memory** | 16 GB RAM |
| **Storage** | 100 GB SSD |
| **Network** | 1 Gbps |
| **OS** | Linux (Ubuntu 20.04+, RHEL 8+) |

</details>

<details>
<summary><strong>ğŸ“‹ Recommended Requirements (Production)</strong></summary>

| Resource | Specification |
|----------|---------------|
| **CPU** | 16+ cores |
| **Memory** | 64+ GB RAM |
| **Storage** | 1+ TB SSD |
| **Network** | 10 Gbps |
| **OS** | Linux (Ubuntu 22.04, RHEL 9) |

</details>

---

## ğŸ”’ Security Architecture

DBLOCK is built with enterprise-grade security features:

### Security Features

| Feature | Description |
|---------|-------------|
| **Encryption at Rest** | All sensitive data and credentials encrypted using AES-256 |
| **Encryption in Transit** | TLS 1.3 for all network communications |
| **Role-Based Access Control** | Granular permissions for users, teams, and pipelines |
| **Data Masking** | Automatic masking of sensitive data in logs and previews |
| **Audit Logging** | Comprehensive logging of all system activities and data access |
| **SSO Integration** | Support for SAML 2.0 and OIDC authentication |

### Data Governance

| Feature | Description |
|---------|-------------|
| **Data Lineage** | Track data flow from source to destination across all pipelines |
| **Schema Evolution** | Manage schema changes with backward compatibility |
| **Data Quality Rules** | Define and enforce data quality constraints |
| **Compliance Controls** | Built-in support for GDPR, CCPA, and industry regulations |

---

## ğŸ“– Related Documentation

| Document | Description |
|----------|-------------|
| [Overview](README.md) | Product overview, features, and benefits |
| [Features](FEATURES.md) | Detailed feature breakdown and capabilities |
| [Roadmap](ROADMAP.md) | Development timeline and planned features |

---

## ğŸ”— External Resources

<p align="center">
  <a href="https://github.com/digitranslab/dblock"><img src="https://img.shields.io/badge/GitHub-Repository-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub Repository"></a>
  <a href="https://discord.com/invite/2mK6h9rp"><img src="https://img.shields.io/badge/Discord-Community-5865F2?style=for-the-badge&logo=discord&logoColor=white" alt="Discord"></a>
</p>

---

<p align="center">
  <a href="README.md">ğŸ“– Back to Overview</a> â€¢
  <a href="../../PRODUCTS.md">ğŸ“¦ Back to Products</a> â€¢
  <a href="../../README.md">ğŸ  Back to Home</a>
</p>

<p align="center">
  <sub>Â© 2024 DigiTransLab. All rights reserved.</sub>
</p>
